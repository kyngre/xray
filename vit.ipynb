{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33956b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f06569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"/home/kangkr1002/facial_bone/Training\"\n",
    "VAL_PATH   = \"/home/kangkr1002/facial_bone/Validation\"\n",
    "TEST_PATH  = \"/home/kangkr1002/facial_bone/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0291e271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c728d8d41944429a4741e642bfcd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2cadacfd90497f9dd3fed745e51539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import torch\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ViT ëª¨ë¸ ë¡œë”© (pretrained)\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=2  # ë¶„ë¥˜í•  í´ë˜ìŠ¤ ìˆ˜ (ex: binary classification)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c94e4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9d2cdc8f8743b496caf5ab3a4e48cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# ViT ì „ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# ìƒˆ transform ì •ì˜\n",
    "def transform_vit(example):\n",
    "    # ì˜ˆì œëŠ” PIL ì´ë¯¸ì§€ì—¬ì•¼ í•´\n",
    "    inputs = feature_extractor(images=example, return_tensors=\"pt\")\n",
    "    return inputs['pixel_values'].squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be2f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path, transform):\n",
    "        self.dataset = datasets.ImageFolder(folder_path)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì •ì˜\n",
    "train_ds = CustomDataset(TRAIN_PATH, transform=transform_vit)\n",
    "val_ds   = CustomDataset(VAL_PATH, transform=transform_vit)\n",
    "test_ds  = CustomDataset(TEST_PATH, transform=transform_vit)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87d05bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTForImageClassification\n",
    "\n",
    "# 1. Config ë§Œë“¤ê¸°\n",
    "config = ViTConfig(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_labels=2,          # ë„ˆê°€ ë¶„ë¥˜í•  í´ë˜ìŠ¤ ìˆ˜\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    qkv_bias=True,\n",
    ")\n",
    "\n",
    "# 2. ëª¨ë¸ ìƒˆë¡œ ë§Œë“¤ê¸° (pretrained ì—†ì´!)\n",
    "model = ViTForImageClassification(config)\n",
    "\n",
    "# 3. ë””ë°”ì´ìŠ¤ ì˜¬ë¦¬ê¸°\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80971731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb7343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.2093, Train Acc = 0.9149 | Val Loss = 0.2508, Val Acc = 0.8970\n",
      "Epoch 2: Train Loss = 0.1720, Train Acc = 0.9314 | Val Loss = 0.2560, Val Acc = 0.8950\n",
      "Epoch 3: Train Loss = 0.1480, Train Acc = 0.9424 | Val Loss = 0.2812, Val Acc = 0.8790\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://10.125.208.184:5000\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"VIT4\"):  # ì „ì²´ í•™ìŠµì„ í•˜ë‚˜ì˜ runìœ¼ë¡œ ë¬¶ëŠ”ë‹¤\n",
    "    mlflow.log_param(\"learning_rate\", optimizer.param_groups[0]['lr'])\n",
    "    mlflow.log_param(\"batch_size\", train_loader.batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", 10)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # ========== Train ==========\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # ========== Validation ==========\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # ========== Log to MLflow ==========\n",
    "        mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", accuracy, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss = {avg_loss:.4f}, Train Acc = {accuracy:.4f} | \"\n",
    "              f\"Val Loss = {avg_val_loss:.4f}, Val Acc = {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f486188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ mlflowì— ì €ì¥\n",
    "mlflow.pytorch.log_model(model, \"model\", registered_model_name=\"VIT4\")\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "torch.save(model.state_dict(), \"vit_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4549de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run VIT3 at: http://10.125.208.184:5000/#/experiments/0/runs/8990d17a96744cb6a7aaf72ea4a464b9\n",
      "ğŸ§ª View experiment at: http://10.125.208.184:5000/#/experiments/0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: '/tmp/tmpwe9k1ha_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# ========== ì´ì–´ì„œ ê¸°ë¡ ==========\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_id\u001b[38;5;241m=\u001b[39mrun_id):\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# 1. ëª¨ë¸ ì €ì¥\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# 2. Validation ë°ì´í„°ë¡œ confusion matrix / classification report ë§Œë“¤ê¸°\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/micromamba/envs/capston-pytorch/lib/python3.10/site-packages/mlflow/pytorch/__init__.py:296\u001b[0m, in \u001b[0;36mlog_model\u001b[0;34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, requirements_file, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mLog a PyTorch model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    PyTorch logged models\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    295\u001b[0m pickle_module \u001b[38;5;241m=\u001b[39m pickle_module \u001b[38;5;129;01mor\u001b[39;00m mlflow_pytorch_pickle_module\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpytorch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconda_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconda_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_example\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequirements_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequirements_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/capston-pytorch/lib/python3.10/site-packages/mlflow/models/model.py:825\u001b[0m, in \u001b[0;36mModel.log\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _validate_and_get_model_config_from_file\n\u001b[1;32m    824\u001b[0m registered_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TempDir() \u001b[38;5;28;01mas\u001b[39;00m tmp:\n\u001b[1;32m    826\u001b[0m     local_path \u001b[38;5;241m=\u001b[39m tmp\u001b[38;5;241m.\u001b[39mpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/capston-pytorch/lib/python3.10/site-packages/mlflow/utils/file_utils.py:411\u001b[0m, in \u001b[0;36mTempDir.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[43mcreate_tmp_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path)\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chdr:\n",
      "File \u001b[0;32m~/micromamba/envs/capston-pytorch/lib/python3.10/site-packages/mlflow/utils/file_utils.py:881\u001b[0m, in \u001b[0;36mcreate_tmp_dir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    878\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(directory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mmkdtemp(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mdirectory)\n\u001b[0;32m--> 881\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdtemp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/capston-pytorch/lib/python3.10/tempfile.py:384\u001b[0m, in \u001b[0;36mmkdtemp\u001b[0;34m(suffix, prefix, dir)\u001b[0m\n\u001b[1;32m    382\u001b[0m _sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempfile.mkdtemp\u001b[39m\u001b[38;5;124m\"\u001b[39m, file)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     \u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0o700\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/tmp/tmpwe9k1ha_'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "os.environ['TMPDIR'] = '/home/kmj388/my_tmp'  # ì—¬ìœ  ê³µê°„ ìˆëŠ” ê²½ë¡œë¡œ ì§€ì •\n",
    "os.makedirs('/home/kmj388/my_tmp', exist_ok=True)\n",
    "# ========== ì„¸íŒ… ==========\n",
    "mlflow.set_tracking_uri(\"http://10.125.208.184:5000\")\n",
    "\n",
    "# ========== ë°©ê¸ˆ í›ˆë ¨í•œ runì˜ IDë¥¼ ê°€ì ¸ì™€ì•¼ í•¨ ==========\n",
    "# ë³´í†µì€ start_run() ì•ˆì—ì„œ run_idë¥¼ ë°”ë¡œ ì €ì¥í•´ë†“ëŠ”ê²Œ ë² ìŠ¤íŠ¸ì˜€ì–´\n",
    "# ì¼ë‹¨ ì—¬ê¸°ì„œëŠ” MLflow UIì—ì„œ run_idë¥¼ ì§ì ‘ ê°€ì ¸ì˜¤ê±°ë‚˜\n",
    "# ë”°ë¡œ ì €ì¥í–ˆë˜ run_idë¥¼ ì—¬ê¸°ì— ë„£ì\n",
    "run_id = \"8990d17a96744cb6a7aaf72ea4a464b9\"  # <<< VIT_2 run idë¡œ ë°”ê¿”\n",
    "\n",
    "# ========== í˜„ì¬ ì—´ë¦° run ë‹«ê¸° ==========\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# ========== ì´ì–´ì„œ ê¸°ë¡ ==========\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "\n",
    "    # 1. ëª¨ë¸ ì €ì¥\n",
    "    mlflow.pytorch.log_model(model, artifact_path=\"model\")\n",
    "\n",
    "    # 2. Validation ë°ì´í„°ë¡œ confusion matrix / classification report ë§Œë“¤ê¸°\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images).logits\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Validation Confusion Matrix')\n",
    "    plt.savefig(\"val_confusion_matrix.png\")\n",
    "    mlflow.log_artifact(\"val_confusion_matrix.png\")\n",
    "\n",
    "    # Classification Report\n",
    "    cls_report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "    with open(\"val_classification_report.json\", \"w\") as f:\n",
    "        json.dump(cls_report, f, indent=4)\n",
    "    mlflow.log_artifact(\"val_classification_report.json\")\n",
    "\n",
    "    # 3. Model Registry ë“±ë¡\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    mlflow.register_model(model_uri=model_uri, name=\"VIT_2_Model\")\n",
    "\n",
    "    # 4. Test ë°ì´í„°ì…‹ë„ ëŒë¦¬ê¸° (ì„ íƒì‚¬í•­)\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:  # â— test_loader ì¤€ë¹„ë˜ì–´ ìˆì–´ì•¼ í•¨\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += preds.eq(labels).sum().item()\n",
    "\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / test_total\n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê¸°ë¡\n",
    "    mlflow.log_metric(\"test_loss\", avg_test_loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ Confusion Matrix\n",
    "    cm_test = confusion_matrix(test_labels, test_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_test)\n",
    "    disp.plot()\n",
    "    plt.title('Test Confusion Matrix')\n",
    "    plt.savefig(\"test_confusion_matrix.png\")\n",
    "    mlflow.log_artifact(\"test_confusion_matrix.png\")\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ Classification Report\n",
    "    test_cls_report = classification_report(test_labels, test_preds, output_dict=True)\n",
    "    with open(\"test_classification_report.json\", \"w\") as f:\n",
    "        json.dump(test_cls_report, f, indent=4)\n",
    "    mlflow.log_artifact(\"test_classification_report.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4384091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ee4d011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì €ì¥ pth\n",
    "torch.save(model.state_dict(), \"vit_model.pth\")\n",
    "\n",
    "#model test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images).logits\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        print(\"Predictions:\", preds)\n",
    "        print(\"Labels:\", labels)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ec3a534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0518\n",
      "Test Accuracy: 0.9840\n",
      "\n",
      "Confusion Matrix:\n",
      "[[495   5]\n",
      " [ 11 489]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9783    0.9900    0.9841       500\n",
      "           1     0.9899    0.9780    0.9839       500\n",
      "\n",
      "    accuracy                         0.9840      1000\n",
      "   macro avg     0.9841    0.9840    0.9840      1000\n",
      "weighted avg     0.9841    0.9840    0.9840      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        logits = outputs.logits   # âœ… ì—¬ê¸° ìˆ˜ì •!\n",
    "        \n",
    "        loss = criterion(logits, labels)  # âœ… logitsë§Œ ë„£ê¸°\n",
    "\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)  # âœ… logits ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡\n",
    "\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += preds.eq(labels).sum().item()\n",
    "\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / test_total\n",
    "test_accuracy = test_correct / test_total\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "cls_report = classification_report(test_labels, test_preds, digits=4)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(cls_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
